{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlaADK-NR452"
      },
      "source": [
        "<h1>\n",
        "  <b>[STEP 1]</b> SAVE BOTH OF THESE FOLDERS AS <u>SHORTCUTS</u> TO YOUR \"MY DRIVE\":\n",
        "  <a href=\"https://drive.google.com/drive/folders/1695PT8xzD3LyDRcd3fTocM7uEBVMm-gW?usp=sharing\" target=\"_blank\">Specimen Images</a> &\n",
        "  <a href=\"https://drive.google.com/drive/folders/1hV1xIqXvEzKdtaawIy-H4K-9SbmWZwoy?usp=drive_link\" target=\"_blank\">Segmented Images</a>\n",
        "</h1>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pV24fXaXevx"
      },
      "source": [
        "**CLONE REPO:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6S_TavKqWOs7",
        "outputId": "6e968a2d-86ab-4067-86bf-9c1f46da31e0"
      },
      "outputs": [],
      "source": [
        "%git clone https://github.com/jescuti/deepplant.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUANsQHaX9JC",
        "outputId": "4e5d09bf-fcb6-4325-97db-012981112594"
      },
      "outputs": [],
      "source": [
        "%cd deepplant\n",
        "%cd preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04x4VIi6Axxi",
        "outputId": "444f6c1f-92d9-44ab-8730-6799e4a5d386"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrNWoO2a_NDG"
      },
      "source": [
        "# Segment Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfvbYVG9Yr5y"
      },
      "source": [
        "**DOWNLOAD SAM:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TE0VO-7rYGyy",
        "outputId": "97b1fe56-8d8b-4cf5-d4ea-49791b146b2d"
      },
      "outputs": [],
      "source": [
        "%pip install -q 'git+https://github.com/facebookresearch/segment-anything.git'\n",
        "%pip install -q jupyter_bbox_widget roboflow dataclasses-json supervision==0.23.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SH6wTAtdauLB"
      },
      "outputs": [],
      "source": [
        "%mkdir -p {HOME}/weights\n",
        "%wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth -P {HOME}/weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFbBM3R3KtFs"
      },
      "source": [
        "**OPEN IMAGES:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeqcLQ5rEyYQ",
        "outputId": "f36db224-177a-433a-c8f7-bdcc93a7c192"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "####################\n",
        "# CONNECT TO DRIVE #\n",
        "####################\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "zip_file_path = '/content/drive/MyDrive/herbarium_images.zip'\n",
        "extract_path = '/content/drive/MyDrive/herbarium_images'\n",
        "\n",
        "###########################\n",
        "# UNZIP ONLY IF NEEDED    #\n",
        "###########################\n",
        "if not os.path.exists(extract_path) or len(os.listdir(extract_path)) == 0:\n",
        "    print(\"Extracting zip file\")\n",
        "    os.makedirs(extract_path, exist_ok=True)\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "    print(f\"Unzipped to {extract_path}\")\n",
        "else:\n",
        "    print(f\"Already extracted at {extract_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vbUwVzAMlEi"
      },
      "source": [
        "**RUN SAM LABEL SEGMENTATION:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BE8n4gRYYxJK",
        "outputId": "1314ffa4-e028-493c-8343-98c37a16c583"
      },
      "outputs": [],
      "source": [
        "%pip install opencv-python torch segment-anything glob tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWG3QUbuAEk7",
        "outputId": "a29b576c-2c64-4ae6-812c-4bee8ef522ac"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "import gc\n",
        "from segment_anything import SamAutomaticMaskGenerator, sam_model_registry\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "\n",
        "# clear cache\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# save memory w/ lower precision (can go up to 'highest' or 'high')\n",
        "torch.set_float32_matmul_precision('medium')\n",
        "\n",
        "##############\n",
        "# SET UP SAM #\n",
        "##############\n",
        "sam_checkpoint = os.path.join(\"{HOME}\", \"weights\", \"sam_vit_h_4b8939.pth\")\n",
        "model_type = \"vit_h\"\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print(\"Setting up SAM!\")\n",
        "sam = sam_model_registry[model_type](checkpoint = sam_checkpoint)\n",
        "sam.to(device = device)\n",
        "\n",
        "mask_generator = SamAutomaticMaskGenerator(\n",
        "    model = sam,\n",
        "    points_per_side = 32,\n",
        "    pred_iou_thresh = 0.7,\n",
        "    stability_score_offset = 0.7,\n",
        "    crop_n_layers = 1,\n",
        "    crop_n_points_downscale_factor = 2,\n",
        "    min_mask_region_area = 50,\n",
        "    output_mode = \"binary_mask\"\n",
        ")\n",
        "\n",
        "image_folder = \"/content/drive/MyDrive/herbarium_images\"\n",
        "# image_folder = \"/content/drive/MyDrive/webscraped_images\"\n",
        "image_paths = glob(os.path.join(image_folder, '**', '*.jpg'), recursive = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sloNdq6MkGUa"
      },
      "outputs": [],
      "source": [
        "##########################################\n",
        "# BARCODE AND HEADER DETECTION FUNCTIONS #\n",
        "##########################################\n",
        "\n",
        "def analyze_background_color(image):\n",
        "    \"\"\"\n",
        "    Analyze image to determine if it has a dark background (most of the\n",
        "    specimen headers do and we don't want to falsely detect those)\n",
        "    Returns `true` if image has mostly dark background\n",
        "    \"\"\"\n",
        "    # grayscale\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # calc histogram\n",
        "    hist = cv2.calcHist([gray], [0], None, [256], [0, 256])\n",
        "\n",
        "    # darkness distribution (counts pixels w/ intensity < 50)\n",
        "    dark_pixels = np.sum(hist[:50])\n",
        "    total_pixels = gray.shape[0] * gray.shape[1]\n",
        "\n",
        "    # of more than 40% of pixels are very dark,\n",
        "    # consider it a \"dark background\"\n",
        "    dark_ratio = dark_pixels / total_pixels\n",
        "\n",
        "    return dark_ratio > 0.4\n",
        "\n",
        "\n",
        "#############################################\n",
        "# CITATION: Rosebrock, A. (2014)            #\n",
        "# \"The Ultimate Guide to Barcode Detection\" #\n",
        "#############################################\n",
        "def detect_barcode(image):\n",
        "    \"\"\"\n",
        "    Detect typical barcode patterns using gradients\n",
        "    Returns True if prob a barcode\n",
        "    \"\"\"\n",
        "    # grayscale\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # gradients in x direction (barcodes have strong horizontal gradients)\n",
        "    # then, scaling to 8b\n",
        "    # (Rosebrock pg. 4)\n",
        "    gradX = cv2.Sobel(gray, ddepth = cv2.CV_32F, dx = 1, dy = 0, ksize = -1)\n",
        "    gradX = np.absolute(gradX)\n",
        "\n",
        "    (minVal, maxVal) = (np.min(gradX), np.max(gradX))\n",
        "    if maxVal == minVal:\n",
        "        return False\n",
        "\n",
        "    gradX = (255 * ((gradX - minVal) / (maxVal - minVal))).astype(\"uint8\")\n",
        "\n",
        "    # threshold and morphology operations (Rosebrock pg. 6)\n",
        "    gradX = cv2.morphologyEx(gradX, cv2.MORPH_CLOSE, cv2.getStructuringElement(cv2.MORPH_RECT, (21, 7)))\n",
        "    thresh = cv2.threshold(gradX, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]\n",
        "\n",
        "    # count horizontal lines\n",
        "    horizontal_count = np.sum(thresh > 0, axis = 1)\n",
        "    max_horizontal = np.max(horizontal_count) if len(horizontal_count) > 0 else 0\n",
        "\n",
        "    # look for dense horizontal lines\n",
        "    return max_horizontal > image.shape[1] * 0.5\n",
        "\n",
        "def is_likely_header_or_barcode(cropped_img, y_position, img_height):\n",
        "    \"\"\"\n",
        "    Determines if a region is likely a header / barcode:\n",
        "    1. position in the image (headers at the top)\n",
        "    2. barcode detection\n",
        "    3. black background detection (herbarium img headers)\n",
        "    \"\"\"\n",
        "    # 1. check position if in top 20% of img\n",
        "    position_ratio = y_position / img_height\n",
        "\n",
        "    # check for dark background\n",
        "    gray = cv2.cvtColor(cropped_img, cv2.COLOR_BGR2GRAY)\n",
        "    mean_intensity = np.mean(gray)\n",
        "\n",
        "    # dark background = likely a header\n",
        "    has_dark_background = mean_intensity < 100\n",
        "\n",
        "    if position_ratio < 0.2 and has_dark_background:\n",
        "        return True\n",
        "\n",
        "    if position_ratio < 0.2:\n",
        "        # additional checks for top regions\n",
        "        # 2. look for barcode-like properties\n",
        "        if detect_barcode(cropped_img):\n",
        "            return True\n",
        "\n",
        "        # 3. density\n",
        "\n",
        "        ##########################################################\n",
        "        # CITATION: Murzova, A. (2020)                           #\n",
        "        # https://learnopencv.com/otsu-thresholding-with-opencv/ #\n",
        "        ##########################################################\n",
        "        _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
        "\n",
        "        h, w = cropped_img.shape[:2]\n",
        "        aspect_ratio = w / h if h > 0 else 0\n",
        "\n",
        "        pixels_per_row = np.sum(binary > 128, axis = 1)\n",
        "        std_dev = np.std(pixels_per_row)\n",
        "        mean = np.mean(pixels_per_row)\n",
        "\n",
        "        # headers have uniform density across rows\n",
        "        uniformity = std_dev / (mean + 1e-5)\n",
        "\n",
        "        # headers and barcodes = high density and low variance\n",
        "        if (aspect_ratio > 3.5 and uniformity < 0.5) or detect_barcode(cropped_img):\n",
        "            return True\n",
        "\n",
        "    elif has_dark_background:\n",
        "        # text-on-dark-background\n",
        "        bright_pixel_count = np.sum(gray > 200)\n",
        "        bright_pixel_ratio = bright_pixel_count / (cropped_img.shape[0] * cropped_img.shape[1])\n",
        "\n",
        "        # check for bright text\n",
        "        if 0.05 < bright_pixel_ratio < 0.4:\n",
        "            return True\n",
        "\n",
        "    return False\n",
        "\n",
        "############################\n",
        "# TEXT DETECTION FUNCTIONS #\n",
        "############################\n",
        "def detect_text_regions(image):\n",
        "    \"\"\"\n",
        "    Identify potential text regions (usually rectangular)\n",
        "    \"\"\"\n",
        "    ######################################################################\n",
        "    # CITATIONS: Rosebrock, A. (2021)                                    #\n",
        "    # https://learnopencv.com/otsu-thresholding-with-opencv/             #\n",
        "    # Yadav, A. (2024)                                                   #\n",
        "    # https://medium.com/%40amit25173/opencv-text-detection-8e298e2b5218 #\n",
        "    ######################################################################\n",
        "\n",
        "    # grayscale\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
        "                                  cv2.THRESH_BINARY_INV, 11, 2)\n",
        "\n",
        "    # kernel\n",
        "    rect_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (15, 3))\n",
        "\n",
        "    # connect text characters\n",
        "    dilation = cv2.dilate(thresh, rect_kernel, iterations=1)\n",
        "\n",
        "    # find contours\n",
        "    contours, _ = cv2.findContours(dilation, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    text_regions = []\n",
        "    for contour in contours:\n",
        "        x, y, w, h = cv2.boundingRect(contour)\n",
        "\n",
        "        # aspect ratio: text labels tend to be wider than they are tall\n",
        "        aspect_ratio = w / float(h)\n",
        "\n",
        "        # filter by area\n",
        "        area = w * h\n",
        "\n",
        "        # label aspect ratio > 1.5\n",
        "        if aspect_ratio > 1.5 and 1000 < area < 100000:\n",
        "            text_regions.append((x, y, w, h))\n",
        "\n",
        "    return text_regions\n",
        "\n",
        "def is_rectangular(mask, threshold = 0.75):\n",
        "    \"\"\"\n",
        "    Check if mask is approx. rectangular by comparing its area\n",
        "    with the area of its bounding box\n",
        "    \"\"\"\n",
        "    # get mask area\n",
        "    mask_area = np.sum(mask)\n",
        "\n",
        "    # get bounding box\n",
        "    y_indices, x_indices = np.where(mask)\n",
        "    if len(y_indices) == 0 or len(x_indices) == 0:\n",
        "        return False\n",
        "\n",
        "    x_min, x_max = np.min(x_indices), np.max(x_indices)\n",
        "    y_min, y_max = np.min(y_indices), np.max(y_indices)\n",
        "\n",
        "    box_width = x_max - x_min + 1\n",
        "    box_height = y_max - y_min + 1\n",
        "    box_area = box_width * box_height\n",
        "\n",
        "    # fullness ratio\n",
        "    fullness = mask_area / box_area\n",
        "\n",
        "    # check aspect ratio for rectangule\n",
        "    aspect_ratio = box_width / max(box_height, 1)\n",
        "\n",
        "    return fullness > threshold and aspect_ratio > 1.5\n",
        "\n",
        "def has_text_characteristics(img):\n",
        "    \"\"\"\n",
        "    See if it has characteristics of a text label rather than barcode/header\n",
        "    \"\"\"\n",
        "    # grayscale\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # check for dark background\n",
        "    mean_intensity = np.mean(gray)\n",
        "    if mean_intensity < 100:\n",
        "        bright_pixels = np.sum(gray > 200)\n",
        "        bright_ratio = bright_pixels / (img.shape[0] * img.shape[1])\n",
        "        if 0.05 < bright_ratio < 0.4:\n",
        "            return False\n",
        "\n",
        "    # calc variance of pixel values (text regions = higher variance)\n",
        "    var = np.var(gray)\n",
        "\n",
        "    # Canny edge detection to analyze edge patterns\n",
        "    edges = cv2.Canny(gray, 100, 200)\n",
        "    edge_density = np.sum(edges > 0) / (img.shape[0] * img.shape[1])\n",
        "\n",
        "    # calc connected components\n",
        "    _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
        "    num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(thresh, connectivity=8)\n",
        "\n",
        "    # check for text-like properties\n",
        "    has_multiple_components = num_labels > 5\n",
        "    reasonable_edge_density = 0.03 < edge_density < 0.15\n",
        "    high_variance = var > 500\n",
        "\n",
        "    # count components with reasonable size\n",
        "    reasonable_components = 0\n",
        "    for i in range(1, num_labels):\n",
        "        component_area = stats[i, cv2.CC_STAT_AREA]\n",
        "        if 10 < component_area < 500:\n",
        "            reasonable_components += 1\n",
        "\n",
        "    has_reasonable_components = reasonable_components > 3\n",
        "\n",
        "    # combine criteria\n",
        "    return high_variance and (has_multiple_components or has_reasonable_components or reasonable_edge_density)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfCRzz9C_1C5",
        "outputId": "64c7e7e9-9064-4b48-9ca9-76de28ad1293"
      },
      "outputs": [],
      "source": [
        "########################\n",
        "# MAIN PROCESSING LOOP #\n",
        "########################\n",
        "output_folder = \"/content/drive/MyDrive/segmented_images\"\n",
        "os.makedirs(output_folder, exist_ok = True)\n",
        "\n",
        "def resize_if_needed(image, max_dim = 1500):\n",
        "    \"\"\"Resize image if it's too large for memory\"\"\"\n",
        "    h, w = image.shape[:2]\n",
        "    if max(h, w) > max_dim:\n",
        "        scale = max_dim / max(h, w)\n",
        "        new_h, new_w = int(h * scale), int(w * scale)\n",
        "        return cv2.resize(image, (new_w, new_h))\n",
        "    return image\n",
        "\n",
        "def process_single_image(img_path):\n",
        "    base_name = os.path.basename(img_path).split('.')[0]\n",
        "    image_id = base_name.split('_')[-1]\n",
        "\n",
        "    # SKIP if image already cropped\n",
        "    existing_crops = glob(os.path.join(output_folder, f\"{image_id}_*.jpg\"))\n",
        "    if existing_crops:\n",
        "        print(f\"Skipping: {img_path}\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        # read image\n",
        "        print(f\"Processing image: {img_path}\")\n",
        "        img = cv2.imread(img_path)\n",
        "        if img is None:\n",
        "            print(f\"Failed to load: {img_path}\")\n",
        "            return\n",
        "\n",
        "        ##############################################\n",
        "        # RESIZE LARGE IMAGES BC OUT OF MEMORY ERROR #\n",
        "        ##############################################\n",
        "        img = resize_if_needed(img)\n",
        "        image_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        img_height = img.shape[0]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            masks = mask_generator.generate(image_rgb)\n",
        "\n",
        "        saved_count = 0\n",
        "        skipped_count = 0\n",
        "\n",
        "        # process SAM masks\n",
        "        # focus on rectangular shapes\n",
        "        for i, mask in enumerate(masks):\n",
        "            if is_rectangular(mask[\"segmentation\"]):\n",
        "                x, y, w, h = mask[\"bbox\"]\n",
        "\n",
        "                # filter by reasonable size for text labels\n",
        "                if 1000 < mask[\"area\"] < 100000:\n",
        "                    padding = 10\n",
        "                    x_pad = max(0, x - padding)\n",
        "                    y_pad = max(0, y - padding)\n",
        "                    w_pad = min(img.shape[1] - x_pad, w + 2*padding)\n",
        "                    h_pad = min(img.shape[0] - y_pad, h + 2*padding)\n",
        "\n",
        "                    cropped_img = img[y_pad:y_pad+h_pad, x_pad:x_pad+w_pad]\n",
        "\n",
        "                    # check if header / barcode\n",
        "                    if is_likely_header_or_barcode(cropped_img, y_pad, img_height) or analyze_background_color(cropped_img):\n",
        "                        skipped_count += 1\n",
        "                        continue\n",
        "\n",
        "                    # check for text-like characteristics\n",
        "                    if has_text_characteristics(cropped_img):\n",
        "                        cropped_img_name = f\"{image_id}_{i}.jpg\"\n",
        "\n",
        "                        #######################\n",
        "                        # SAVE CROPPED IMAGES #\n",
        "                        #######################\n",
        "                        save_path = os.path.join(output_folder, cropped_img_name)\n",
        "                        cv2.imwrite(save_path, cropped_img)\n",
        "                        saved_count += 1\n",
        "                    else:\n",
        "                        skipped_count += 1\n",
        "\n",
        "        print(f\"Total regions saved: {saved_count}; Skipped: {skipped_count}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {img_path}: {str(e)}\")\n",
        "\n",
        "    # clean up to free memory\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# prevent memory accumulation => small batches\n",
        "batch_size = 1\n",
        "for i in tqdm(range(4754, len(image_paths), batch_size)):\n",
        "    batch = image_paths[i:i+batch_size]\n",
        "    for img_path in batch:\n",
        "        process_single_image(img_path)\n",
        "\n",
        "    # garbage collection between batches\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D29ZjTSWCMZn"
      },
      "source": [
        "# Image Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4t1NuLSIJJL5",
        "outputId": "b8caf00b-2290-4532-a5e7-1435f3a9ab29"
      },
      "outputs": [],
      "source": [
        "%pip install -U scikit-learn fiftyone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y31IJickJMnb",
        "outputId": "1b93c741-11d5-416d-f698-6956579ac2b8"
      },
      "outputs": [],
      "source": [
        "%fiftyone plugins download https://github.com/jacobmarks/clustering-plugin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1KAuc-HJOOZ",
        "outputId": "627f0ca0-d315-4aad-a91f-1811fd740639"
      },
      "outputs": [],
      "source": [
        "%pip install umap-learn git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-g93EnOCJXx1"
      },
      "outputs": [],
      "source": [
        "import fiftyone as fo\n",
        "import fiftyone.brain as fob\n",
        "from fiftyone import ViewField as F\n",
        "import os\n",
        "from PIL import Image\n",
        "import clip\n",
        "import torch\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# takes in a directory to the segmented labels\n",
        "# returns clustered dataset\n",
        "def cluster_dataset(segmented_labels_dir):\n",
        "   image_files = [os.path.join(segmented_labels_dir, f) for f in os.listdir(segmented_labels_dir) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
        "   dataset = fo.Dataset.from_images(image_files, name='labels_test', overwrite=True)\n",
        "   session = fo.launch_app(dataset)\n",
        "   # compute features\n",
        "   res = fob.compute_visualization(\n",
        "        dataset,\n",
        "        model=\"clip-vit-base32-torch\",\n",
        "        embeddings=\"clip_embeddings\",\n",
        "        method=\"umap\",\n",
        "        brain_key=\"clip_vis\",\n",
        "        batch_size=10\n",
        "    )\n",
        "   dataset.set_values(\"clip_umap\", res.current_points)\n",
        "   return dataset\n",
        "\n",
        "# takes in a path to the search image and dataset, as well as k, the number of results to return\n",
        "# returns top k similar images to the search image, as well as their similarity scores\n",
        "def query_image(image_path, dataset, k):\n",
        "   model_name = \"ViT-B/32\"\n",
        "   device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "   #get image embeddings\n",
        "   model, preprocess = clip.load(model_name, device=device)\n",
        "   image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
        "   with torch.no_grad():\n",
        "      image_embedding = model.encode_image(image).cpu().numpy()\n",
        "\n",
        "   #get dataset embeddings\n",
        "   dataset_embeddings = dataset.values(\"clip_embeddings\")\n",
        "   sample_ids = dataset.values(\"id\")\n",
        "\n",
        "   #calculate similarity\n",
        "   similarity_scores = cosine_similarity(image_embedding, np.array(dataset_embeddings))[0]\n",
        "\n",
        "   #get top k most similar images\n",
        "   sorted_indices = np.argsort(similarity_scores)[::-1]\n",
        "   top_k_indices = sorted_indices[:k]\n",
        "\n",
        "   top_similar_images = []\n",
        "   top_similarity_scores = []\n",
        "   for i in top_k_indices:\n",
        "      sample = dataset[sample_ids[i]]\n",
        "      img = Image.open(sample.filepath)\n",
        "      plt.imshow(img)\n",
        "      plt.show()\n",
        "      top_similar_images.append(img)\n",
        "      top_similarity_scores.append(similarity_scores[i])\n",
        "\n",
        "   return top_similar_images, similarity_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install torch torchvision flask flask-cors pillow numpy scikit-learn fiftyone umap-learn\n",
        "%pip install git+https://github.com/openai/CLIP.git\n",
        "%pip install -q jupyter_bbox_widget roboflow dataclasses-json supervision==0.23.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLUKqYXI-1wG"
      },
      "source": [
        "# Web Scrape Images (Optional)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWOSSg-EAlMv"
      },
      "source": [
        "**Collects 500 Random Images from the Collection**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvEHzzqM_Irf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import re\n",
        "import time\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from bs4 import BeautifulSoup\n",
        "import shutil\n",
        "\n",
        "save_path = '/content/drive/MyDrive/webscraped_images'\n",
        "os.makedirs(save_path, exist_ok = True)\n",
        "\n",
        "def create_filename(name):\n",
        "    # create a valid filename\n",
        "    return \"\".join(c if c.isalnum() or c in \" ._-\" else \"_\" for c in name).strip()\n",
        "\n",
        "def extract_image_url_from_html(html_content):\n",
        "    \"\"\"\n",
        "    extract the actual image URL from the HTML content\n",
        "    \"\"\"\n",
        "    try:\n",
        "        soup = BeautifulSoup(html_content, 'html.parser')\n",
        "        # IIIF => web address with the image, metadata,\n",
        "        # and the image \"web viewer\"\n",
        "        script_tags = soup.find_all('script')\n",
        "        for script in script_tags:\n",
        "            if script.string and 'tileSources' in script.string:\n",
        "                # extract URL using regex\n",
        "                match = re.search(r'\"(https://repository\\.library\\.brown\\.edu/iiif/image/bdr:[^/]+/info\\.json)\"', script.string)\n",
        "                if match:\n",
        "                    iiif_info_url = match.group(1)\n",
        "                    # convert info.json URL to full image URL\n",
        "                    return iiif_info_url.replace('/info.json', '/full/full/0/default.jpg')\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing HTML: {e}\")\n",
        "        return None\n",
        "\n",
        "def download_image(url, filepath):\n",
        "    \"\"\"\n",
        "    download an image from the URL and save to filepath\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, stream = True)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        with open(filepath, 'wb') as f:\n",
        "            for chunk in response.iter_content(8192):\n",
        "                f.write(chunk)\n",
        "\n",
        "        if os.path.getsize(filepath) == 0:\n",
        "            print(f\"WARNING: Downloaded file is empty: {filepath}\")\n",
        "            return False\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading image: {e}\")\n",
        "        return False\n",
        "\n",
        "def get_total_item_count():\n",
        "    \"\"\"\n",
        "    get the total number of items in the collection\n",
        "    \"\"\"\n",
        "    api_url = (\n",
        "        \"https://repository.library.brown.edu/api/search/\"\n",
        "        \"?q=rel_is_member_of_collection_ssim:bdr:nz9qn2kb\"\n",
        "        \"&rows=1&wt=json\"\n",
        "    )\n",
        "    try:\n",
        "        response = requests.get(api_url).json()\n",
        "        return response.get(\"response\", {}).get(\"numFound\", 0)\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting total item count: {e}\")\n",
        "        return 0\n",
        "\n",
        "def fetch_random_sample(sample_size = 100, min_index = 4401, base_dir = \"test_images\"):\n",
        "    \"\"\"\n",
        "    fetch a random sample of herbarium images\n",
        "    and save them to folders\n",
        "\n",
        "    starts at 4,401 because we already downloaded all\n",
        "    the images before that #\n",
        "    \"\"\"\n",
        "    os.makedirs(base_dir, exist_ok = True)\n",
        "\n",
        "    total_items = get_total_item_count()\n",
        "    if total_items == 0:\n",
        "        print(\"ERROR: couldn't determine the total number of items in the collection.\")\n",
        "        return 0, 0\n",
        "\n",
        "    print(f\"collection has {total_items} TOTAL items\")\n",
        "\n",
        "    # calculate available range for random sampling\n",
        "    available_range = total_items - min_index\n",
        "    if available_range <= 0:\n",
        "        print(\"no more items available for sampling\")\n",
        "        return 0, 0\n",
        "\n",
        "    # generate random indices\n",
        "    if sample_size > available_range:\n",
        "        print(f\"Requested sample size {sample_size} exceeds available items {available_range}.\")\n",
        "        sample_size = available_range\n",
        "\n",
        "    random_indices = random.sample(range(min_index, total_items), sample_size)\n",
        "\n",
        "    total_processed = 0\n",
        "    total_errors = 0\n",
        "\n",
        "    # process each random index\n",
        "    for index in tqdm(random_indices, desc = \"Processing random samples\"):\n",
        "        # fetch single item at the random index\n",
        "        api_url = (\n",
        "            \"https://repository.library.brown.edu/api/search/\"\n",
        "            \"?q=rel_is_member_of_collection_ssim:bdr:nz9qn2kb\"\n",
        "            f\"&start={index}&rows=1&wt=json\"\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            response = requests.get(api_url).json()\n",
        "            docs = response.get(\"response\", {}).get(\"docs\", [])\n",
        "\n",
        "            if not docs:\n",
        "                print(f\"no item found at INDEX: {index}\")\n",
        "                continue\n",
        "\n",
        "            # process item\n",
        "            item = docs[0]\n",
        "            pid = item.get(\"pid\")\n",
        "            if not pid:\n",
        "                continue\n",
        "\n",
        "            # get collector name / unknown if not available\n",
        "            collector = item.get(\"dwc_recorded_by_ssi\", \"Unknown_Collector\")\n",
        "            collector = create_filename(collector)\n",
        "\n",
        "            # scientific name for image name, if available\n",
        "            scientific_name = item.get(\"dwc_scientific_name_ssi\", \"\")\n",
        "            scientific_name = create_filename(scientific_name) if scientific_name else \"\"\n",
        "\n",
        "            # make title for file name\n",
        "            title = item.get(\"primary_title\", \"Untitled\")\n",
        "            title = create_filename(title)\n",
        "\n",
        "            # ceate folder for that collector\n",
        "            # collector_dir = os.path.join(base_dir, collector)\n",
        "            # os.makedirs(collector_dir, exist_ok = True)\n",
        "\n",
        "            # make the image filename\n",
        "            if scientific_name:\n",
        "                filename = f\"{scientific_name}_{pid.replace(':', '_')}.jpg\"\n",
        "            else:\n",
        "                filename = f\"{title}_{pid.replace(':', '_')}.jpg\"\n",
        "\n",
        "            # filepath = os.path.join(collector_dir, filename)\n",
        "            filepath = os.path.join(base_dir, filename)\n",
        "\n",
        "            # SKIP if file exists\n",
        "            if os.path.exists(filepath):\n",
        "                print(f\"File already exists: {filepath}\")\n",
        "                continue\n",
        "\n",
        "            # create URL to open the image view => then, fetch the HTML\n",
        "            viewer_url = f\"https://repository.library.brown.edu/viewers/image/zoom/{pid}\"\n",
        "            try:\n",
        "                print(f\"Fetching HTML from {viewer_url}\")\n",
        "                html_response = requests.get(viewer_url)\n",
        "                html_response.raise_for_status()\n",
        "\n",
        "                # extract image URL from HTML\n",
        "                image_url = extract_image_url_from_html(html_response.text)\n",
        "                if not image_url:\n",
        "                    # direct IIIF URL\n",
        "                    image_url = f\"https://repository.library.brown.edu/iiif/image/{pid}/full/full/0/default.jpg\"\n",
        "                    print(f\"Using alternative URL: {image_url}\")\n",
        "\n",
        "                # download actual image as JPG\n",
        "                if download_image(image_url, filepath):\n",
        "                    print(f\"Successfully downloaded {filename} to {save_path}\")\n",
        "                    total_processed += 1\n",
        "                else:\n",
        "                    print(f\"Failed to download image for {pid}\")\n",
        "                    total_errors += 1\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {pid}: {e}\")\n",
        "                total_errors += 1\n",
        "\n",
        "            # don't overwhelm server\n",
        "            time.sleep(0.5)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching item at index {index}: {e}\")\n",
        "            total_errors += 1\n",
        "            time.sleep(5)\n",
        "\n",
        "    print(f\"Successfully processed {total_processed} images with ERRORS: {total_errors}\")\n",
        "    return total_processed, total_errors\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # download 500 random samples from the remaining items (after index 4400)\n",
        "    fetch_random_sample(sample_size = 500, min_index = 4401, base_dir = save_path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "tLUKqYXI-1wG"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
