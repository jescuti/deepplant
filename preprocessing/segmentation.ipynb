{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is intended to be run in a Google Colab file :)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>\n",
    "  <b>[STEP 1]</b> SAVE BOTH OF THESE FOLDERS AS <u>SHORTCUTS</u> TO YOUR \"MY DRIVE\":\n",
    "  <a href=\"https://drive.google.com/drive/folders/1695PT8xzD3LyDRcd3fTocM7uEBVMm-gW?usp=sharing\" target=\"_blank\">Specimen Images</a> &\n",
    "  <a href=\"https://drive.google.com/drive/folders/1hV1xIqXvEzKdtaawIy-H4K-9SbmWZwoy?usp=drive_link\" target=\"_blank\">Segmented Images</a>\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DOWNLOAD SAM:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q 'git+https://github.com/facebookresearch/segment-anything.git'\n",
    "%pip install -q jupyter_bbox_widget roboflow dataclasses-json supervision==0.23.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mkdir -p {HOME}/weights\n",
    "%wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth -P {HOME}/weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OPEN IMAGES:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "####################\n",
    "# CONNECT TO DRIVE #\n",
    "####################\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "zip_file_path = '/content/drive/MyDrive/herbarium_images.zip'\n",
    "extract_path = '/content/drive/MyDrive/herbarium_images'\n",
    "\n",
    "###########################\n",
    "# UNZIP ONLY IF NEEDED    #\n",
    "###########################\n",
    "if not os.path.exists(extract_path) or len(os.listdir(extract_path)) == 0:\n",
    "    print(\"Extracting zip file\")\n",
    "    os.makedirs(extract_path, exist_ok=True)\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_path)\n",
    "    print(f\"Unzipped to {extract_path}\")\n",
    "else:\n",
    "    print(f\"Already extracted at {extract_path}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RUN SAM LABEL SEGMENTATION:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install opencv-python torch segment-anything glob tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import gc\n",
    "from segment_anything import SamAutomaticMaskGenerator, sam_model_registry\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "# clear cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# save memory w/ lower precision (can go up to 'highest' or 'high')\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "##############\n",
    "# SET UP SAM #\n",
    "##############\n",
    "sam_checkpoint = os.path.join(\"{HOME}\", \"weights\", \"sam_vit_h_4b8939.pth\")\n",
    "model_type = \"vit_h\"\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(\"Setting up SAM!\")\n",
    "sam = sam_model_registry[model_type](checkpoint = sam_checkpoint)\n",
    "sam.to(device = device)\n",
    "\n",
    "mask_generator = SamAutomaticMaskGenerator(\n",
    "    model = sam,\n",
    "    points_per_side = 32,\n",
    "    pred_iou_thresh = 0.7,\n",
    "    stability_score_offset = 0.7,\n",
    "    crop_n_layers = 1,\n",
    "    crop_n_points_downscale_factor = 2,\n",
    "    min_mask_region_area = 50,\n",
    "    output_mode = \"binary_mask\"\n",
    ")\n",
    "\n",
    "image_folder = \"/content/drive/MyDrive/herbarium_images\"\n",
    "# image_folder = \"/content/drive/MyDrive/webscraped_images\"\n",
    "image_paths = glob(os.path.join(image_folder, '**', '*.jpg'), recursive = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "# BARCODE AND HEADER DETECTION FUNCTIONS #\n",
    "##########################################\n",
    "\n",
    "def analyze_background_color(image):\n",
    "    \"\"\"\n",
    "    Analyze image to determine if it has a dark background (most of the\n",
    "    specimen headers do and we don't want to falsely detect those)\n",
    "    Returns `true` if image has mostly dark background\n",
    "    \"\"\"\n",
    "    # grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # calc histogram\n",
    "    hist = cv2.calcHist([gray], [0], None, [256], [0, 256])\n",
    "\n",
    "    # darkness distribution (counts pixels w/ intensity < 50)\n",
    "    dark_pixels = np.sum(hist[:50])\n",
    "    total_pixels = gray.shape[0] * gray.shape[1]\n",
    "\n",
    "    # of more than 40% of pixels are very dark,\n",
    "    # consider it a \"dark background\"\n",
    "    dark_ratio = dark_pixels / total_pixels\n",
    "\n",
    "    return dark_ratio > 0.4\n",
    "\n",
    "\n",
    "#############################################\n",
    "# CITATION: Rosebrock, A. (2014)            #\n",
    "# \"The Ultimate Guide to Barcode Detection\" #\n",
    "#############################################\n",
    "def detect_barcode(image):\n",
    "    \"\"\"\n",
    "    Detect typical barcode patterns using gradients\n",
    "    Returns True if prob a barcode\n",
    "    \"\"\"\n",
    "    # grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # gradients in x direction (barcodes have strong horizontal gradients)\n",
    "    # then, scaling to 8b\n",
    "    # (Rosebrock pg. 4)\n",
    "    gradX = cv2.Sobel(gray, ddepth = cv2.CV_32F, dx = 1, dy = 0, ksize = -1)\n",
    "    gradX = np.absolute(gradX)\n",
    "\n",
    "    (minVal, maxVal) = (np.min(gradX), np.max(gradX))\n",
    "    if maxVal == minVal:\n",
    "        return False\n",
    "\n",
    "    gradX = (255 * ((gradX - minVal) / (maxVal - minVal))).astype(\"uint8\")\n",
    "\n",
    "    # threshold and morphology operations (Rosebrock pg. 6)\n",
    "    gradX = cv2.morphologyEx(gradX, cv2.MORPH_CLOSE, cv2.getStructuringElement(cv2.MORPH_RECT, (21, 7)))\n",
    "    thresh = cv2.threshold(gradX, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]\n",
    "\n",
    "    # count horizontal lines\n",
    "    horizontal_count = np.sum(thresh > 0, axis = 1)\n",
    "    max_horizontal = np.max(horizontal_count) if len(horizontal_count) > 0 else 0\n",
    "\n",
    "    # look for dense horizontal lines\n",
    "    return max_horizontal > image.shape[1] * 0.5\n",
    "\n",
    "def is_likely_header_or_barcode(cropped_img, y_position, img_height):\n",
    "    \"\"\"\n",
    "    Determines if a region is likely a header / barcode:\n",
    "    1. position in the image (headers at the top)\n",
    "    2. barcode detection\n",
    "    3. black background detection (herbarium img headers)\n",
    "    \"\"\"\n",
    "    # 1. check position if in top 20% of img\n",
    "    position_ratio = y_position / img_height\n",
    "\n",
    "    # check for dark background\n",
    "    gray = cv2.cvtColor(cropped_img, cv2.COLOR_BGR2GRAY)\n",
    "    mean_intensity = np.mean(gray)\n",
    "\n",
    "    # dark background = likely a header\n",
    "    has_dark_background = mean_intensity < 100\n",
    "\n",
    "    if position_ratio < 0.2 and has_dark_background:\n",
    "        return True\n",
    "\n",
    "    if position_ratio < 0.2:\n",
    "        # additional checks for top regions\n",
    "        # 2. look for barcode-like properties\n",
    "        if detect_barcode(cropped_img):\n",
    "            return True\n",
    "\n",
    "        # 3. density\n",
    "\n",
    "        ##########################################################\n",
    "        # CITATION: Murzova, A. (2020)                           #\n",
    "        # https://learnopencv.com/otsu-thresholding-with-opencv/ #\n",
    "        ##########################################################\n",
    "        _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "\n",
    "        h, w = cropped_img.shape[:2]\n",
    "        aspect_ratio = w / h if h > 0 else 0\n",
    "\n",
    "        pixels_per_row = np.sum(binary > 128, axis = 1)\n",
    "        std_dev = np.std(pixels_per_row)\n",
    "        mean = np.mean(pixels_per_row)\n",
    "\n",
    "        # headers have uniform density across rows\n",
    "        uniformity = std_dev / (mean + 1e-5)\n",
    "\n",
    "        # headers and barcodes = high density and low variance\n",
    "        if (aspect_ratio > 3.5 and uniformity < 0.5) or detect_barcode(cropped_img):\n",
    "            return True\n",
    "\n",
    "    elif has_dark_background:\n",
    "        # text-on-dark-background\n",
    "        bright_pixel_count = np.sum(gray > 200)\n",
    "        bright_pixel_ratio = bright_pixel_count / (cropped_img.shape[0] * cropped_img.shape[1])\n",
    "\n",
    "        # check for bright text\n",
    "        if 0.05 < bright_pixel_ratio < 0.4:\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "############################\n",
    "# TEXT DETECTION FUNCTIONS #\n",
    "############################\n",
    "def detect_text_regions(image):\n",
    "    \"\"\"\n",
    "    Identify potential text regions (usually rectangular)\n",
    "    \"\"\"\n",
    "    ######################################################################\n",
    "    # CITATIONS: Rosebrock, A. (2021)                                    #\n",
    "    # https://learnopencv.com/otsu-thresholding-with-opencv/             #\n",
    "    # Yadav, A. (2024)                                                   #\n",
    "    # https://medium.com/%40amit25173/opencv-text-detection-8e298e2b5218 #\n",
    "    ######################################################################\n",
    "\n",
    "    # grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                  cv2.THRESH_BINARY_INV, 11, 2)\n",
    "\n",
    "    # kernel\n",
    "    rect_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (15, 3))\n",
    "\n",
    "    # connect text characters\n",
    "    dilation = cv2.dilate(thresh, rect_kernel, iterations=1)\n",
    "\n",
    "    # find contours\n",
    "    contours, _ = cv2.findContours(dilation, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    text_regions = []\n",
    "    for contour in contours:\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "\n",
    "        # aspect ratio: text labels tend to be wider than they are tall\n",
    "        aspect_ratio = w / float(h)\n",
    "\n",
    "        # filter by area\n",
    "        area = w * h\n",
    "\n",
    "        # label aspect ratio > 1.5\n",
    "        if aspect_ratio > 1.5 and 1000 < area < 100000:\n",
    "            text_regions.append((x, y, w, h))\n",
    "\n",
    "    return text_regions\n",
    "\n",
    "def is_rectangular(mask, threshold = 0.75):\n",
    "    \"\"\"\n",
    "    Check if mask is approx. rectangular by comparing its area\n",
    "    with the area of its bounding box\n",
    "    \"\"\"\n",
    "    # get mask area\n",
    "    mask_area = np.sum(mask)\n",
    "\n",
    "    # get bounding box\n",
    "    y_indices, x_indices = np.where(mask)\n",
    "    if len(y_indices) == 0 or len(x_indices) == 0:\n",
    "        return False\n",
    "\n",
    "    x_min, x_max = np.min(x_indices), np.max(x_indices)\n",
    "    y_min, y_max = np.min(y_indices), np.max(y_indices)\n",
    "\n",
    "    box_width = x_max - x_min + 1\n",
    "    box_height = y_max - y_min + 1\n",
    "    box_area = box_width * box_height\n",
    "\n",
    "    # fullness ratio\n",
    "    fullness = mask_area / box_area\n",
    "\n",
    "    # check aspect ratio for rectangule\n",
    "    aspect_ratio = box_width / max(box_height, 1)\n",
    "\n",
    "    return fullness > threshold and aspect_ratio > 1.5\n",
    "\n",
    "def has_text_characteristics(img):\n",
    "    \"\"\"\n",
    "    See if it has characteristics of a text label rather than barcode/header\n",
    "    \"\"\"\n",
    "    # grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # check for dark background\n",
    "    mean_intensity = np.mean(gray)\n",
    "    if mean_intensity < 100:\n",
    "        bright_pixels = np.sum(gray > 200)\n",
    "        bright_ratio = bright_pixels / (img.shape[0] * img.shape[1])\n",
    "        if 0.05 < bright_ratio < 0.4:\n",
    "            return False\n",
    "\n",
    "    # calc variance of pixel values (text regions = higher variance)\n",
    "    var = np.var(gray)\n",
    "\n",
    "    # Canny edge detection to analyze edge patterns\n",
    "    edges = cv2.Canny(gray, 100, 200)\n",
    "    edge_density = np.sum(edges > 0) / (img.shape[0] * img.shape[1])\n",
    "\n",
    "    # calc connected components\n",
    "    _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "    num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(thresh, connectivity=8)\n",
    "\n",
    "    # check for text-like properties\n",
    "    has_multiple_components = num_labels > 5\n",
    "    reasonable_edge_density = 0.03 < edge_density < 0.15\n",
    "    high_variance = var > 500\n",
    "\n",
    "    # count components with reasonable size\n",
    "    reasonable_components = 0\n",
    "    for i in range(1, num_labels):\n",
    "        component_area = stats[i, cv2.CC_STAT_AREA]\n",
    "        if 10 < component_area < 500:\n",
    "            reasonable_components += 1\n",
    "\n",
    "    has_reasonable_components = reasonable_components > 3\n",
    "\n",
    "    # combine criteria\n",
    "    return high_variance and (has_multiple_components or has_reasonable_components or reasonable_edge_density)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "# MAIN PROCESSING LOOP #\n",
    "########################\n",
    "output_folder = \"/content/drive/MyDrive/segmented_images\"\n",
    "os.makedirs(output_folder, exist_ok = True)\n",
    "\n",
    "def resize_if_needed(image, max_dim = 1500):\n",
    "    \"\"\"Resize image if it's too large for memory\"\"\"\n",
    "    h, w = image.shape[:2]\n",
    "    if max(h, w) > max_dim:\n",
    "        scale = max_dim / max(h, w)\n",
    "        new_h, new_w = int(h * scale), int(w * scale)\n",
    "        return cv2.resize(image, (new_w, new_h))\n",
    "    return image\n",
    "\n",
    "def process_single_image(img_path):\n",
    "    base_name = os.path.basename(img_path).split('.')[0]\n",
    "    image_id = base_name.split('_')[-1]\n",
    "\n",
    "    # SKIP if image already cropped\n",
    "    existing_crops = glob(os.path.join(output_folder, f\"{image_id}_*.jpg\"))\n",
    "    if existing_crops:\n",
    "        print(f\"Skipping: {img_path}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # read image\n",
    "        print(f\"Processing image: {img_path}\")\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            print(f\"Failed to load: {img_path}\")\n",
    "            return\n",
    "\n",
    "        ##############################################\n",
    "        # RESIZE LARGE IMAGES BC OUT OF MEMORY ERROR #\n",
    "        ##############################################\n",
    "        img = resize_if_needed(img)\n",
    "        image_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img_height = img.shape[0]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            masks = mask_generator.generate(image_rgb)\n",
    "\n",
    "        saved_count = 0\n",
    "        skipped_count = 0\n",
    "\n",
    "        # process SAM masks\n",
    "        # focus on rectangular shapes\n",
    "        for i, mask in enumerate(masks):\n",
    "            if is_rectangular(mask[\"segmentation\"]):\n",
    "                x, y, w, h = mask[\"bbox\"]\n",
    "\n",
    "                # filter by reasonable size for text labels\n",
    "                if 1000 < mask[\"area\"] < 100000:\n",
    "                    padding = 10\n",
    "                    x_pad = max(0, x - padding)\n",
    "                    y_pad = max(0, y - padding)\n",
    "                    w_pad = min(img.shape[1] - x_pad, w + 2*padding)\n",
    "                    h_pad = min(img.shape[0] - y_pad, h + 2*padding)\n",
    "\n",
    "                    cropped_img = img[y_pad:y_pad+h_pad, x_pad:x_pad+w_pad]\n",
    "\n",
    "                    # check if header / barcode\n",
    "                    if is_likely_header_or_barcode(cropped_img, y_pad, img_height) or analyze_background_color(cropped_img):\n",
    "                        skipped_count += 1\n",
    "                        continue\n",
    "\n",
    "                    # check for text-like characteristics\n",
    "                    if has_text_characteristics(cropped_img):\n",
    "                        cropped_img_name = f\"{image_id}_{i}.jpg\"\n",
    "\n",
    "                        #######################\n",
    "                        # SAVE CROPPED IMAGES #\n",
    "                        #######################\n",
    "                        save_path = os.path.join(output_folder, cropped_img_name)\n",
    "                        cv2.imwrite(save_path, cropped_img)\n",
    "                        saved_count += 1\n",
    "                    else:\n",
    "                        skipped_count += 1\n",
    "\n",
    "        print(f\"Total regions saved: {saved_count}; Skipped: {skipped_count}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {img_path}: {str(e)}\")\n",
    "\n",
    "    # clean up to free memory\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# prevent memory accumulation => small batches\n",
    "batch_size = 1\n",
    "for i in tqdm(range(4754, len(image_paths), batch_size)):\n",
    "    batch = image_paths[i:i+batch_size]\n",
    "    for img_path in batch:\n",
    "        process_single_image(img_path)\n",
    "\n",
    "    # garbage collection between batches\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7500c3e1c7c786e4ba1e4b4eb7588219b4e35d5153674f92eb3a82672b534f6e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
